#!/usr/bin/env python3
# core/wcag_importer.py
import json, re, io, os
from datetime import datetime
from typing import List, Dict, Any, Tuple, Union
from pathlib import Path

import pandas as pd

BASE = Path(__file__).parent.parent.resolve()
WCAG_LIB_DIR = BASE / "wcag_lib"
WCAG_LIB_DIR.mkdir(parents=True, exist_ok=True)

_SC_RE = re.compile(r"(\d)\.(\d)\.(\d)")

# ---- helpers ---------------------------------------------------------------

def _norm_sc(raw: Union[str, float, int]) -> str:
    """Normalize SC values like 1.1.1 / 'wcag111' / 'SC-1.1.1' → '1.1.1'. Returns '' if not matched."""
    if raw is None:
        return ""
    s = str(raw).strip()
    # common patterns
    m = _SC_RE.search(s)
    if m:
        return ".".join(m.groups())
    m = re.search(r"wcag(\d)(\d)(\d)", s, flags=re.I)
    if m:
        return ".".join(m.groups())
    s = s.replace("SC-", "").replace("sc-", "")
    m = _SC_RE.search(s)
    return ".".join(m.groups()) if m else ""

def _tags_for_sc(sc: str) -> List[str]:
    if not sc:
        return []
    compact = sc.replace(".", "")
    return [f"SC-{sc}", sc, f"wcag{compact}"]

def _split_list(cell) -> List[str]:
    """Split cell content into list by newline / bullet / semicolon / comma."""
    if cell is None or (isinstance(cell, float) and pd.isna(cell)):
        return []
    s = str(cell).strip()
    if not s:
        return []
    # normalize bullets to newlines
    s = s.replace("•", "\n").replace("·", "\n").replace("– ", "\n").replace("— ", "\n")
    parts = re.split(r"[\n;]+|(?<!http[s]?):\s|,(?!\s?http)", s)  # be gentle with URLs
    return [p.strip() for p in parts if p and p.strip()]

def _coalesce(*names) -> str:
    """Return the first non-empty string."""
    for n in names:
        if n and str(n).strip():
            return str(n).strip()
    return ""

def _read_excel(xlsx: Union[str, Path, bytes, io.BytesIO]) -> pd.DataFrame:
    if isinstance(xlsx, (bytes, io.BytesIO)):
        return pd.read_excel(xlsx, sheet_name=0)
    return pd.read_excel(str(xlsx), sheet_name=0)

def _merge_lists(a: List[str], b: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in (a or []) + (b or []):
        x = (x or "").strip()
        if not x or x in seen:
            continue
        seen.add(x); out.append(x)
    return out

# ---- main ------------------------------------------------------------------

def import_wcag_from_excel(
    xlsx: Union[str, Path, bytes, io.BytesIO],
    out_dir: Path = WCAG_LIB_DIR,
    merge_existing: bool = True
) -> Dict[str, Any]:
    """
    Read an Excel with WCAG technique rows and write/patch JSON files to wcag_lib/sc-*.json.

    Expected columns (case/spacing-insensitive; aliases supported):
      - SC (e.g., 1.1.1)
      - Topic / Title (optional)
      - Do
      - Dont / Don't
      - Edge cases / Edge_Cases
      - Techniques (e.g., H37; G94; ARIA6)
      - Axe rules / Axe_Rules / axe_rules_hint (rule ids, comma/semicolon/newline)
      - Tags (optional; will be autogenerated if missing)

    Returns: {"created": N, "updated": M, "skipped": K, "files": [paths...]}
    """
    df = _read_excel(xlsx)
    # normalize columns
    colmap = {}
    for c in df.columns:
        k = re.sub(r"[\s_\-]+", "", str(c).lower())
        colmap[k] = c
    def getcol(*keys):
        for k in keys:
            if k in colmap:
                return colmap[k]
        return None

    sc_col    = getcol("sc", "successcriterion", "criterion")
    topic_col = getcol("topic", "title", "name")
    do_col    = getcol("do", "dos", "doitems")
    dont_col  = getcol("dont", "don't", "donts")
    edge_col  = getcol("edgecases", "edge", "edges", "edge_case", "edgecase")
    tech_col  = getcol("techniques", "technique", "refs")
    axe_col   = getcol("axerules", "axe_rules", "axe_rules_hint", "axehint", "axehints", "axeruleids")
    tags_col  = getcol("tags")

    created, updated, skipped = 0, 0, 0
    written_paths = []

    for _, row in df.iterrows():
        sc = _norm_sc(row.get(sc_col)) if sc_col else ""
        if not sc:
            skipped += 1
            continue

        topic = _coalesce(row.get(topic_col), f"SC {sc}") if topic_col else f"SC {sc}"

        do_list   = _split_list(row.get(do_col))   if do_col   else []
        dont_list = _split_list(row.get(dont_col)) if dont_col else []
        edge_list = _split_list(row.get(edge_col)) if edge_col else []
        techs     = [t.strip() for t in _split_list(row.get(tech_col))] if tech_col else []
        axe_hints = [t.strip() for t in _split_list(row.get(axe_col))]  if axe_col  else []

        tags = _split_list(row.get(tags_col)) if tags_col else []
        # ensure standard tags are present
        tags = _merge_lists(tags, _tags_for_sc(sc))

        doc = {
            "topic": topic,
            "tags": tags,
            "do": do_list,
            "dont": dont_list,
            "edge_cases": edge_list,
            "techniques": techs,
            "axe_rules_hint": axe_hints,
            "updated_at": datetime.utcnow().isoformat(timespec="seconds") + "Z"
        }

        out_path = out_dir / f"sc-{sc}.json"

        if out_path.exists() and merge_existing:
            try:
                existing = json.loads(out_path.read_text(encoding="utf-8"))
            except Exception:
                existing = {}
            # merge conservatively: keep existing text, extend lists, refresh timestamp
            merged = {
                "topic": _coalesce(doc.get("topic"), existing.get("topic")),
                "tags": _merge_lists(existing.get("tags", []), doc.get("tags", [])),
                "do": _merge_lists(existing.get("do", []), doc.get("do", [])),
                "dont": _merge_lists(existing.get("dont", []), doc.get("dont", [])),
                "edge_cases": _merge_lists(existing.get("edge_cases", []), doc.get("edge_cases", [])),
                "techniques": _merge_lists(existing.get("techniques", []), doc.get("techniques", [])),
                "axe_rules_hint": _merge_lists(existing.get("axe_rules_hint", []), doc.get("axe_rules_hint", [])),
                "updated_at": doc["updated_at"]
            }
            out_path.write_text(json.dumps(merged, indent=2, ensure_ascii=False), encoding="utf-8")
            updated += 1
        else:
            out_path.write_text(json.dumps(doc, indent=2, ensure_ascii=False), encoding="utf-8")
            created += 1

        written_paths.append(str(out_path))

    return {"created": created, "updated": updated, "skipped": skipped, "files": written_paths}

# Simple CLI usage: python -m core.wcag_importer path/to/WCAG_Checklist.xlsx
if __name__ == "__main__":
    import sys
    xlsx = sys.argv[1] if len(sys.argv) > 1 else None
    if not xlsx:
        print("Usage: python -m core.wcag_importer <excel_file>")
        sys.exit(2)
    res = import_wcag_from_excel(xlsx)
    print(json.dumps(res, indent=2))
